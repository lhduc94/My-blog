{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title: Applying logistic regression in Python\n",
    "---\n",
    "\n",
    "...from lessons learned from Andrew Ng's [ML course](https://www.youtube.com/watch?v=-la3q9d7AKQ). Like other assignments of the course, the logistic regression assignment used MATLAB. Here, I also translate MATLAB code into Python. I apply the mathematical concepts and the Python code to a datset I evaluated in another [project](https://benslack19.github.io/projects/predicting_states_sc_analysis/#supervised-machine-learning). Check out the project link for more context to this post."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import packages and data from project notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I followed this StackOverflow [post](https://stackoverflow.com/questions/31621414/share-data-between-ipython-notebooks) to store and transfer the data from my [activation state project](https://benslack19.github.io/projects/predicting_states_sc_analysis/#supervised-machine-learning) into this Jupyter notebook and post."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no stored variable # This is the entire data\n",
      "no stored variable # This is the activation state label.\n"
     ]
    }
   ],
   "source": [
    "# Import data\n",
    "%store -r X   # This is the entire data  \n",
    "%store -r y   # This is the activation state label."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I chose two genes that showed reasonable, but not wide, separation between the groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gene1 = 'Plk2'\n",
    "gene2 = 'Ankrd33b'\n",
    "X = X.loc[:,[gene1, gene2]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic regression functions\n",
    "\n",
    "I use the naming conventions of the Ng course. The logistic regression model is such that we want the hypothesis to be within the bounds 0 and 1.\n",
    "\n",
    "$$ 0 \\leq h_{\\theta}(x) \\leq 1$$\n",
    "\n",
    "If $$ h_{\\theta}(x) = g(\\theta^Tx) $$ and $$ g(z) = \\frac{1}{1 + exp(-z)}\\ $$\n",
    "then\n",
    "\n",
    "$$ h_{\\theta}(x) = \\frac{1}{1 + exp(-\\theta^Tx)} $$\n",
    "\n",
    "The objective will be to determine the parameters $\\theta$.\n",
    "\n",
    "To do this, we will minimize the logistic regression cost function\n",
    "\n",
    "$$ J(\\theta) = \\frac{1}{m} \\sum_{i=1}^{m}[-y^{(i)}\\log(h_{\\theta}(x^{(i)}))-(1-y^{(i)})\\log(1-h_{\\theta}(x^{(i)}))] $$\n",
    "\n",
    "The gradient equation is $$ \\frac{\\partial J(\\theta)}{\\partial \\theta_{j}} = \\frac{1}{m} \\sum_{i=1}^{m}(h_{\\theta}(x^{(i)}))-y^{(i)})x^{(i)}_{j} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting\n",
    "We start the exercise by first plotting the top two features of the data. Blue is the negative class, red is the positive class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEKCAYAAAAfGVI8AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x12070f048>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Matplotlib plotting syntax isdifferent from MATLAB\n",
    "f, (ax1) = plt.subplots(1,1);\n",
    "mask = y==0;\n",
    "ax1.scatter(x=X.loc[mask,gene1], y=X.loc[mask,gene2], color='blue', label='neg');\n",
    "mask = y==1;\n",
    "ax1.scatter(x=X.loc[mask,gene1], y=X.loc[mask,gene2], color='red', label='pos');\n",
    "ax1.set_xlabel('gene 1');\n",
    "ax1.set_ylabel('gene 2');\n",
    "ax1.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Cost and Gradient\n",
    "\n",
    "These were the directions in the MATLAB code of the ML course assignment:\n",
    "\"In this part of the exercise, you will implement the cost and gradient\n",
    "for logistic regression. You need to complete the code in costFunction.m.\"\n",
    "\n",
    "I'll make each function a separate Jupyter cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Sigmoid function\n",
    "def sigmoid(z):\n",
    "    return (1 / (1 + np.exp(-z)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Cost function\n",
    "def costFunction(theta, X, y):\n",
    "    # Initialize some useful values\n",
    "    m = len(y);\n",
    "    J = 0;\n",
    "    grad = np.zeros(theta.shape)\n",
    "    \n",
    "    # -- code exercise from class (MATLAB code commented out)--- \n",
    "    # h = sigmoid(X*theta);\n",
    "    h = sigmoid(np.dot(X, theta)).flatten()  # flatten needed from testing\n",
    "    # J = 1/m*((-y'*log(h))-(1-y)'*log(1-h)); \n",
    "    step1 = np.dot(y.T, np.log(h))\n",
    "    step2 = np.dot((1-y).T, np.log(1-h))\n",
    "    J = (1/m)*(-step1-step2);\n",
    "    # grad = 1/m*(X'*(h-y));\n",
    "    grad = (1/m)*(np.dot(X.T,(h-y)));\n",
    "    return (J, grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Setup the data matrix appropriately, and add ones for the intercept term\n",
    "# [m, n] = size(X);\n",
    "(m, n) = X.shape;\n",
    "\n",
    "# Add intercept term to x and X_test\n",
    "# X = [ones(m, 1) X];\n",
    "X.insert(loc=0, column='x0', value=np.ones((m, 1)))\n",
    "\n",
    "# Initialize fitting parameters\n",
    "# initial_theta = zeros(n + 1, 1);\n",
    "initial_theta = np.zeros((n + 1, 1));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost at initial theta (zeros):  0.69314718056\n",
      "Gradient at test theta: \n",
      " [ 0.0890411  -1.32378626 -1.42244151]\n"
     ]
    }
   ],
   "source": [
    "# Compute and display initial cost and gradient\n",
    "# [cost, grad] = costFunction(initial_theta, X, y);\n",
    "(cost, grad) = costFunction(initial_theta, X, y);\n",
    "print('Cost at initial theta (zeros): ', cost);\n",
    "print('Gradient at test theta: \\n', grad);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizing using fmin_tnc for Python (fminunc in MATLAB)\n",
    "\n",
    "Course directions say: \"In this exercise, you will use a built-in function (fminunc) to find the optimal parameters theta.\"\n",
    "\n",
    "Here's where things got tricky for me. I initially wasn't sure what optimization function to use. I was both informed and confused by what I found online, partly because sometimes cost and gradient were calculated in separate functions. In the MATLAB course, the cost and gradient were returned in the same function (```costFunction```) so I did the same. This is what course notes said about using ```fminunc```:\n",
    "\n",
    "*\"In this code snippet, we first defined the options to be used with fminunc. Specifically, we set the GradObj option to on, which tells ```fminunc``` that our function returns both the cost and the gradient. This allows fminunc to use the gradient when minimizing the function.\"*\n",
    "\n",
    "The Python minimizatioon function I settled on using was [```fmin_tnc```](https://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.optimize.fmin_tnc.html) which I chose from looking at [this post](http://www.johnwittenauer.net/machine-learning-exercises-in-python-part-3/). In that example, the author had gradient as a separate function. From looking at the documentation, it looks like ```fmin_tnc``` can handle the optimized function either with or without the gradient being returned. If it's not returned, one has to specify the gradient function in the ```fprime``` parameter of ```fmin_tnc```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import scipy.optimize as opt  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The x0 parameter is the initial guess so I will set that to inital_theta.\n",
    "result = opt.fmin_tnc(func=costFunction, x0=initial_theta, args=(X, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thetas found by fmin_tnc function:  [-7.65282614  0.74252063  0.54925607]\n"
     ]
    }
   ],
   "source": [
    "print('Thetas found by fmin_tnc function: ', result[0]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost at theta found by scipy fmin_tnc:  0.123972041854\n"
     ]
    }
   ],
   "source": [
    "optimal_theta = result[0]\n",
    "(cost, grad) = costFunction(optimal_theta, X, y)\n",
    "print('Cost at theta found by scipy fmin_tnc: ', cost);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting the decision boundary\n",
    "\n",
    "In the course, the MATLAB function was given to us as ```plotDecisionBoundary.m```. Initially, my strategy was going to do a nearly line-for-line translation of the MATLAB code to Python syntax, but since the plotting is quite different, I just ended up testing code and coming up with my own function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Determine the decision boundary linear equation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the lectures:\n",
    "$ h_{\\theta}(x) = g(\\theta^Tx) \\geq 0.5$ when $\\theta^Tx \\geq 0$.\n",
    "\n",
    "In the case of two features:\n",
    "\n",
    "$ h_{\\theta}(x) = g(\\theta_{0} + \\theta_{1}x_{1} + \\theta_{2}x_{2})$\n",
    "\n",
    "Therefore: $\\theta_{0} + \\theta_{1}x_{1} + \\theta_{2}x_{2} \\geq 0$\n",
    "\n",
    "We can re-arrange to solve for the linear equation: $ x_{2} = \\frac{-\\theta_{1}x_{1} - \\theta_{0}}{\\theta_{2}}\\ $\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "slope:  -1.35186604306\n",
      "y-intercept:  13.9330751618\n"
     ]
    }
   ],
   "source": [
    "# Calculations for the decision boundary line equation.\n",
    "slope = -optimal_theta[1]/optimal_theta[2]\n",
    "yint = -optimal_theta[0]/optimal_theta[2]\n",
    "print('slope: ', slope)\n",
    "print('y-intercept: ', yint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The equation of the line is therefore:\n",
    "$$ x_{2} = -1.352x_{1} + 13.933 $$\n",
    "\n",
    "Looking at the plot of data above, this seems like a reasonable equation for the decision boundary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  ```plotDecisionBoundary``` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plotDecisionBoundary(theta, X, y):\n",
    "    \n",
    "    # Plot data\n",
    "    f, (ax1) = plt.subplots(1,1);\n",
    "    mask = y==0;\n",
    "    ax1.scatter(x=X.loc[mask,gene1], y=X.loc[mask,gene2], color='blue', label='neg');\n",
    "    mask = y==1;\n",
    "    ax1.scatter(x=X.loc[mask,gene1], y=X.loc[mask,gene2], color='red', label='pos');\n",
    "    ax1.set_xlabel('gene 1');\n",
    "    ax1.set_ylabel('gene 2');\n",
    "\n",
    "    # Calculations for the decision boundary line equation\n",
    "    slope = -theta[1]/theta[2]\n",
    "    yint = -theta[0]/theta[2]\n",
    "\n",
    "    # Plot the decision boundary\n",
    "    f_line = lambda x: slope*x + yint;\n",
    "\n",
    "    # Use the min and max of x values\n",
    "    x = np.array([min(X.loc[:,gene1]),max(X.loc[:,gene1])]);\n",
    "    ax1.plot(x,f_line(x), c=\"orange\", label=\"decision boundary\");\n",
    "    ax1.set_ylim(bottom=-1)\n",
    "    ax1.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEKCAYAAAAfGVI8AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x120765fd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plotDecisionBoundary(optimal_theta, X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This seems like a very sensible decision boundary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction and Accuracies\n",
    "\n",
    "The ML course instructed us to create a predict function and then use that to compute the accuracy of the predicted classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Prediction function\n",
    "def predict(theta, X):\n",
    "    h = sigmoid(np.dot(X, theta)).flatten()\n",
    "    p = h >= 0.5\n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy:  97.2602739726\n"
     ]
    }
   ],
   "source": [
    "# Compute accuracy on our training set\n",
    "p = predict(optimal_theta, X);\n",
    "print('Train Accuracy: ', np.mean(p == y) * 100);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using scikit-learn's logistic regression and regularization\n",
    "\n",
    "Let's look at the default theta values that are produced when using the [logistic regression function from scikit-learn (SKL)](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) and compare with the thetas we calculated with `fmin_tnc` optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Instantiate a logistic regression model, and fit with X and y \n",
    "model = LogisticRegression()\n",
    "model = model.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thetas from SKL logistic regression:  [-1.93428654  0.36535035  0.35153828]\n",
      "Thetas found by fmin_tnc function:  [-7.65282614  0.74252063  0.54925607]\n"
     ]
    }
   ],
   "source": [
    "print('Thetas from SKL logistic regression: ', model.coef_[0])\n",
    "print('Thetas found by fmin_tnc function: ', result[0]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why are the values different using the SKL approach versus what we determined using our optimization function? If one were to use default parameter values of SKL logistic regression, it might be easy to overlook that L2 (ridge) regularization is used as a default for this function. The optimal thetas we determined above with `fmin_tnc` did not use regularization. While regularization is not necessary in this simplified (two-feature) case, it can be used to minimize overfitting when datasets have many features.\n",
    "\n",
    "The logistic regression cost function with L2 regularization as taught in the ML course is\n",
    "\n",
    "$$ J(\\theta) = \\frac{1}{m} \\sum_{i=1}^{m}[-y^{(i)}\\log(h_{\\theta}(x^{(i)}))-(1-y^{(i)})\\log(1-h_{\\theta}(x^{(i)}))] + \\frac{\\lambda}{2m}\\sum_{j=1}^{n}\\theta^2_{j} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do we turn off regularization in the SKL function? SKL does not allow us to remove regularization directly, but we can nullify the regularization term by setting lambda to zero. The SKL function controls regularization strength by the parameter `C` which is inversely proportional to lambda. Therefore, one must [set the C value to be large](https://stackoverflow.com/questions/25427650/sklearn-logisticregression-without-regularization) to drive the regularization parameter towards 0. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 10, 100, 1000, 10000, 100000]\n"
     ]
    }
   ],
   "source": [
    "c_values = [10**i for i in range(6)]\n",
    "print(c_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C =  1 \t SKL thetas:  [-1.93428654  0.36535035  0.35153828]\n",
      "C =  10 \t SKL thetas:  [-3.18925315  0.6161684   0.4764179 ]\n",
      "C =  100 \t SKL thetas:  [-3.73121761  0.72361884  0.53807019]\n",
      "C =  1000 \t SKL thetas:  [-3.81569535  0.74039178  0.54798173]\n",
      "C =  10000 \t SKL thetas:  [-3.82468845  0.742178    0.54904109]\n",
      "C =  100000 \t SKL thetas:  [-3.82559369  0.7423578   0.54914777]\n",
      "Thetas found by fmin_tnc function:  [-7.65282614  0.74252063  0.54925607]\n"
     ]
    }
   ],
   "source": [
    "for i in c_values:\n",
    "    model_wreg = LogisticRegression(C=i)\n",
    "    model_wreg = model_wreg.fit(X, y)\n",
    "    print('C = ', i, '\\t SKL thetas: ', model_wreg.coef_[0])\n",
    "print('Thetas found by fmin_tnc function: ', result[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the $\\theta_1$ and $\\theta_2$ values match nearly exactly. The $\\theta_0$ values differ by exactly a factor of 2, which is reflective of an [optional factor](https://stats.stackexchange.com/questions/57628/why-is-the-logistic-regression-cost-function-scaled-by-the-number-of-examples) to include in the calculation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Others had translated the MATLAB code to Python for the assignment of the course. Check out those Python implementations I found ([here](http://aimotion.blogspot.com/2011/11/machine-learning-with-python-logistic.html), [here](http://nbviewer.jupyter.org/github/tfolkman/learningwithdata/blob/master/Logistic%20Gradient%20Descent.ipynb), and [here](http://www.johnwittenauer.net/machine-learning-exercises-in-python-part-3/)) which also helped me as I was doing this small project. \n",
    "\n",
    "In this post, I chose a subset of data from a personal project, implemented step-by-step logistic regression by translating MATLAB code to Python, and compared the theta values produced by the optimization function to the theta values derived from scikit-learn's logistic regression function. This provided an opportunity to re-inforce lessons from the Machine Learning course. Seeing how the two methods arrive at the same values also provided cross-confirmation for me.\n",
    "\n",
    "Scikit-learn is an amazing package and for most technical projects, you would not write out your own logistic regression function. But writing the function and comparing values helps make the SKL logistic regression a little bit less of a black box.\n",
    "\n",
    "*This post was originally created on March 18, 2018.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {
    "height": "625px",
    "left": "0px",
    "right": "833px",
    "top": "106px",
    "width": "268px"
   },
   "toc_section_display": "block",
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
